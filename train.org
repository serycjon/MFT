* Training RAFT for MFT
The official checkpoint was trained on Sintel, FlyingThings3D a dataset generated by [[https://github.com/google-research/kubric][Kubric]], but the procedure to generate the data is a bit complicated, so we also share a config without training on Kubric (results not much worse).

** Setting up the datasets
*** Sintel
Place or symlink the [[http://sintel.is.tue.mpg.de/downloads][Sintel dataset]] into =datasets/Sintel-complete=.

The =datasets/Sintel-complete= should have structure something like:
#+BEGIN_EXAMPLE
.
`-- training
    |-- clean
    |   |-- cave_4
    |   |   |-- frame_0001.png
    |   |   `-- frame_0002.png
    |   `-- alley_1
    |       |-- frame_0001.png
    |       `-- frame_0002.png
    |-- final
    |   `-- alley_1
    |       |-- frame_0001.png
    |       `-- frame_0002.png
    |-- flow
    |   `-- ambush_6
    |       |-- frame_0001.flo
    |       `-- frame_0002.flo
    `-- occlusions_rev
        `-- market_2
            |-- frame_0001.png
            `-- frame_0002.png
#+END_EXAMPLE

The =occlusions_rev= is a revised occlusion annotation marking out-of-view pixels as occluded and can be downloaded [[https://download.visinf.tu-darmstadt.de/data/flyingchairs_occ/occlusions_rev.zip][here]].

*** FlyingThings3D
Place or symlink the [[https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html#downloads][FlyingThings3D dataset]] into =datasets/FlyingThings3D=.
Additionally, we have used optical flow occlusion masks, available for download from [[https://drive.google.com/file/d/1RYIkmXwXygrm2SHohFNTLXKhaIfnJuDh/view?usp=drive_link][google drive]] or [[https://cmp.felk.cvut.cz/~serycjon/MFT/materials/flying_things_3D_occlusions.tar.gz][MFT website]] (2.3GB).
The masks were generated a very long time ago by a script which we include in [[generate_occlusion_maps_FlyingThings3D.py]] for documentation.
It may still be working, but we didn't test it recently.
Note that the FlyingThings3D authors also provide occlusion maps in the newer FlyingThings3D dataset subset ("DispNet/FlowNet2.0 dataset subsets" on the download page).
Feel free to create a pull request if you implement the dataloader for the FT3D subset.

The =datasets/FlyingThings3D= should have structure something like:
#+BEGIN_EXAMPLE
.
|-- frames_cleanpass
|   `-- TRAIN
|       `-- A
|           `-- 0003
|               `-- left
|                   |-- 0006.png
|                   |-- 0007.png
|                   `-- 0008.png
|-- optical_flow
|   `-- TRAIN
|       `-- A
|           `-- 0003
|	        |-- into_future
|               |   `-- left
|               |       `-- OpticalFlowIntoFuture_0006_L.pfm
|	        `-- into_past
|                   `-- left
|                       `-- OpticalFlowIntoPast_0006_L.pfm
`-- optical_flow_occlusion_png
    `-- TRAIN
        `-- A
            `-- 0003
	        |-- into_future
                |   `-- left
                |       `-- OpticalFlowIntoFuture_0006_L.png
	        `-- into_past
                    `-- left
                        `-- OpticalFlowIntoPast_0006_L.png
#+END_EXAMPLE

*** Kubric LongFlow
The generated dataset can be downloaded from [[https://drive.google.com/file/d/1-TR49-6JlRfMh-yqNaA8ARd-5kEQnYKl/view?usp=drive_link][google drive]] (42.9GB).
Place it into =datasets/kubric_movi_e_longterm=, it should have structure like:
#+BEGIN_EXAMPLE
.
`-- train
    |-- 00000
    |   |-- images
    |   |   |-- 0000.png
    |   |   |-- 0001.png
    |   |   `-- 0002.png
    |   `-- flowou
    |       |-- 0000_to_0000.flowou.png
    |       |-- 0000_to_0001.flowou.png
    |       `-- 0000_to_0002.flowou.png
    `-- 05794
        |-- images ...
        `-- flowou ...
#+END_EXAMPLE

**** Generating the LongFlow dataset
We generated the dataset from the kubric [[https://github.com/google-research/kubric/tree/e140e24e078d5e641c4ac10bf25743059bd059ce/challenges/movi#movi-e][MOVi-E]] by computing dense flow between the frame =0000= and all the other frames in each sequence.
See the [[file:MFT/RAFT/multiflow_from_kubric.py][multiflow_from_kubric.py]] script that was used to generate the dataset (based on [[https://github.com/google-research/kubric/blob/main/challenges/point_tracking/dataset.py][Kubric Point-Tracking Dataset]].

** Training
We start the training from the =raft-sintel.pth= checkpoint provided by RAFT authors [[https://www.dropbox.com/s/4j4z58wuv8o0mfz/models.zip][here]]. Place it into the =checkpoints/= directory.

Install the dependencies:
#+BEGIN_SRC sh
pip install torchvision tensorboard
#+END_SRC

And run the training:
#+BEGIN_SRC sh
python -m MFT.RAFT.train @train_params.txt
# or python -m MFT.RAFT.train @train_params_no_kubric.txt
#+END_SRC
